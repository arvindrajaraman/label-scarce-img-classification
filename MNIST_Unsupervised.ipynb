{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQnNZ6Otbvfk"
   },
   "source": [
    "# Simulating Unsupervised Image Classification in Label-Scarce Environments\n",
    "\n",
    "In this Jupyter notebook, I attempt to perform MNIST image classification by minimizing the number of calls to labels for images, in order to simulate working an an environment that is scarce of labeled data.\n",
    "\n",
    "First, I use a deep auto-encoder architecture to compress the $768$-dimensional representation of MNIST images to $32$ dimensions and to extract features.\n",
    "\n",
    "Then, I iteratively perform $k$-means clustering. In a certain iteration where we divide the data into $k$ clusters, I find the medoid of each cluster. The medoid of a cluster is the closest datapoint in the dataset to the centroid (\"average\" or numerical center) of that cluster. I query the medoid's label and assign that label to the entire cluster. I specifically query for the medoid's label in a cluster, because datapoints closer to the fringes have a higher chance of being misclassified; the medoid's label is more likely to be representative of the cluster.\n",
    "\n",
    "If for a certain $k$, the desired accuracy is not reached, then $k$ is doubled and we run $k$-means clustering again. The reason why we geometrically increase $k$ (doubling) instead of arithmetically increasing $k$ (adding a fixed value to $k$ in each iteration), because as follows: Let us say that the optimal $k$ where the accuracy is just above or equal to $0.9$ is $k'$. Then, the total images queried is as follows:\n",
    "\n",
    "$\\sum_{i=1}^{\\log_2{k'}} 2^i = 1 + 2 + 4 + \\ldots + k' = 2k' - 1$.\n",
    "\n",
    "In other words, the total images queried will be approximately twice compared to if we knew that $k'$ was the optimal $k$ before-hand (and we could just set $k=k'$ and query for $k'$ images from the start). This is a good bound.\n",
    "\n",
    "In total, I queried for $310$ images' labels. This means that $\\frac{60000-310}{60000}=99.5\\%$ of the dataset doesn't need to be labeled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wuozJ0YBE40"
   },
   "source": [
    "# Loading MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A3j2GdcvBE41",
    "outputId": "b1f610ab-8d46-4971-a79f-7780f2abbe2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyC3Z2yDBE42"
   },
   "source": [
    "# Using an Auto-encoder to Reduce Dimensionality of Image Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "XWg61UPkBE42"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "input_img = keras.Input(shape=(784,))\n",
    "\n",
    "encoded = layers.Dense(256, activation='relu')(input_img)\n",
    "encoded = layers.Dense(128, activation='relu')(encoded)\n",
    "encoded = layers.Dense(64, activation='relu')(encoded)\n",
    "encoded = layers.Dense(32, activation='relu')(encoded)\n",
    "\n",
    "decoded = layers.Dense(64, activation='relu')(encoded)\n",
    "decoded = layers.Dense(128, activation='relu')(decoded)\n",
    "decoded = layers.Dense(256, activation='relu')(decoded)\n",
    "decoded = layers.Dense(784, activation='sigmoid')(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u13U3vciBE42",
    "outputId": "e29a80cd-f764-4e32-b784-6f8b4124f918"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "235/235 [==============================] - 2s 6ms/step - loss: 0.3173 - val_loss: 0.1599\n",
      "Epoch 2/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1514 - val_loss: 0.1336\n",
      "Epoch 3/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1316 - val_loss: 0.1225\n",
      "Epoch 4/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1218 - val_loss: 0.1150\n",
      "Epoch 5/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1148 - val_loss: 0.1110\n",
      "Epoch 6/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1102 - val_loss: 0.1065\n",
      "Epoch 7/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1067 - val_loss: 0.1033\n",
      "Epoch 8/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1041 - val_loss: 0.1017\n",
      "Epoch 9/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1014 - val_loss: 0.0991\n",
      "Epoch 10/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0994 - val_loss: 0.0982\n",
      "Epoch 11/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0978 - val_loss: 0.0964\n",
      "Epoch 12/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0966 - val_loss: 0.0955\n",
      "Epoch 13/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0956 - val_loss: 0.0944\n",
      "Epoch 14/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0944 - val_loss: 0.0935\n",
      "Epoch 15/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0935 - val_loss: 0.0927\n",
      "Epoch 16/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0926 - val_loss: 0.0921\n",
      "Epoch 17/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0918 - val_loss: 0.0911\n",
      "Epoch 18/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0913 - val_loss: 0.0906\n",
      "Epoch 19/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0906 - val_loss: 0.0903\n",
      "Epoch 20/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0899 - val_loss: 0.0894\n",
      "Epoch 21/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0894 - val_loss: 0.0886\n",
      "Epoch 22/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0890 - val_loss: 0.0884\n",
      "Epoch 23/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0884 - val_loss: 0.0879\n",
      "Epoch 24/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0882 - val_loss: 0.0880\n",
      "Epoch 25/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0875 - val_loss: 0.0874\n",
      "Epoch 26/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0870 - val_loss: 0.0878\n",
      "Epoch 27/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0872 - val_loss: 0.0872\n",
      "Epoch 28/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0868 - val_loss: 0.0868\n",
      "Epoch 29/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0863 - val_loss: 0.0868\n",
      "Epoch 30/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0861 - val_loss: 0.0866\n",
      "Epoch 31/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0859 - val_loss: 0.0864\n",
      "Epoch 32/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0856 - val_loss: 0.0858\n",
      "Epoch 33/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0854 - val_loss: 0.0854\n",
      "Epoch 34/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0851 - val_loss: 0.0854\n",
      "Epoch 35/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0849 - val_loss: 0.0850\n",
      "Epoch 36/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0848 - val_loss: 0.0848\n",
      "Epoch 37/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0843 - val_loss: 0.0844\n",
      "Epoch 38/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0841 - val_loss: 0.0845\n",
      "Epoch 39/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0842 - val_loss: 0.0841\n",
      "Epoch 40/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0840 - val_loss: 0.0840\n",
      "Epoch 41/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0838 - val_loss: 0.0844\n",
      "Epoch 42/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0836 - val_loss: 0.0838\n",
      "Epoch 43/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0835 - val_loss: 0.0838\n",
      "Epoch 44/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0833 - val_loss: 0.0837\n",
      "Epoch 45/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0832 - val_loss: 0.0832\n",
      "Epoch 46/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0830 - val_loss: 0.0835\n",
      "Epoch 47/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0829 - val_loss: 0.0830\n",
      "Epoch 48/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0826 - val_loss: 0.0829\n",
      "Epoch 49/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0825 - val_loss: 0.0833\n",
      "Epoch 50/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0826 - val_loss: 0.0825\n",
      "Epoch 51/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0822 - val_loss: 0.0829\n",
      "Epoch 52/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0822 - val_loss: 0.0822\n",
      "Epoch 53/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0820 - val_loss: 0.0825\n",
      "Epoch 54/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0819 - val_loss: 0.0827\n",
      "Epoch 55/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0819 - val_loss: 0.0824\n",
      "Epoch 56/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0815 - val_loss: 0.0820\n",
      "Epoch 57/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0813 - val_loss: 0.0817\n",
      "Epoch 58/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0812 - val_loss: 0.0821\n",
      "Epoch 59/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0810 - val_loss: 0.0815\n",
      "Epoch 60/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0810 - val_loss: 0.0815\n",
      "Epoch 61/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0810 - val_loss: 0.0818\n",
      "Epoch 62/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0810 - val_loss: 0.0819\n",
      "Epoch 63/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0808 - val_loss: 0.0816\n",
      "Epoch 64/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0808 - val_loss: 0.0817\n",
      "Epoch 65/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0805 - val_loss: 0.0811\n",
      "Epoch 66/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0805 - val_loss: 0.0810\n",
      "Epoch 67/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0807 - val_loss: 0.0810\n",
      "Epoch 68/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0801 - val_loss: 0.0809\n",
      "Epoch 69/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0804 - val_loss: 0.0808\n",
      "Epoch 70/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0802 - val_loss: 0.0813\n",
      "Epoch 71/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0803 - val_loss: 0.0814\n",
      "Epoch 72/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0800 - val_loss: 0.0808\n",
      "Epoch 73/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0804 - val_loss: 0.0806\n",
      "Epoch 74/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0802 - val_loss: 0.0807\n",
      "Epoch 75/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0799 - val_loss: 0.0808\n",
      "Epoch 76/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0802 - val_loss: 0.0805\n",
      "Epoch 77/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0800 - val_loss: 0.0805\n",
      "Epoch 78/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0799 - val_loss: 0.0805\n",
      "Epoch 79/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0797 - val_loss: 0.0806\n",
      "Epoch 80/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0796 - val_loss: 0.0805\n",
      "Epoch 81/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0798 - val_loss: 0.0803\n",
      "Epoch 82/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0797 - val_loss: 0.0810\n",
      "Epoch 83/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0797 - val_loss: 0.0805\n",
      "Epoch 84/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0794 - val_loss: 0.0803\n",
      "Epoch 85/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0796 - val_loss: 0.0800\n",
      "Epoch 86/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0795 - val_loss: 0.0805\n",
      "Epoch 87/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0794 - val_loss: 0.0802\n",
      "Epoch 88/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0793 - val_loss: 0.0804\n",
      "Epoch 89/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0792 - val_loss: 0.0801\n",
      "Epoch 90/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0791 - val_loss: 0.0803\n",
      "Epoch 91/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0794 - val_loss: 0.0801\n",
      "Epoch 92/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0792 - val_loss: 0.0803\n",
      "Epoch 93/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0791 - val_loss: 0.0799\n",
      "Epoch 94/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0791 - val_loss: 0.0800\n",
      "Epoch 95/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0791 - val_loss: 0.0797\n",
      "Epoch 96/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0791 - val_loss: 0.0800\n",
      "Epoch 97/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0790 - val_loss: 0.0804\n",
      "Epoch 98/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0791 - val_loss: 0.0800\n",
      "Epoch 99/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0789 - val_loss: 0.0800\n",
      "Epoch 100/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0790 - val_loss: 0.0800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7effc248cdd0>"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Autoencoder (encoder -> decoder)\n",
    "autoencoder = keras.Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Encoder\n",
    "encoder = keras.Model(input_img, encoded)\n",
    "\n",
    "# Training\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=100,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BW0Ryqs-BE44",
    "outputId": "d9624823-99c0-4d8b-a7cd-e1d8b068482a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 32)\n",
      "(10000, 32)\n"
     ]
    }
   ],
   "source": [
    "x_train_comp = encoder.predict(x_train)\n",
    "x_test_comp = encoder.predict(x_test)\n",
    "\n",
    "print(x_train_comp.shape)\n",
    "print(x_test_comp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZjpMIAZN46D",
    "outputId": "849c9a44-76db-4256-fb53-c659bcb0149c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000\n"
     ]
    }
   ],
   "source": [
    "img_to_label = dict()\n",
    "\n",
    "for i in range(x_train_comp.shape[0]):\n",
    "  img_to_label[str(x_train_comp[i])] = y_train[i]\n",
    "\n",
    "for i in range(x_test_comp.shape[0]):\n",
    "  img_to_label[str(x_test_comp[i])] = y_test[i]\n",
    "\n",
    "print(len(img_to_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXKw4LSYDIW4"
   },
   "source": [
    "# Iterative K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pB2vBHSMTrIQ",
    "outputId": "0388cdd6-8fe2-45bb-97fd-48929ea95ce6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K\tAccuracy\n",
      "10\t0.5652\n",
      "20\t0.708\n",
      "40\t0.7996\n",
      "80\t0.8785\n",
      "160\t0.9045\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "print('K\\tAccuracy')\n",
    "\n",
    "k = 10\n",
    "accs = [] # List of tuples (k, accuracy)\n",
    "while True:\n",
    "  # Run k-means clustering with k clusterss\n",
    "  kmeans = KMeans(n_clusters=k, random_state=0).fit(x_train_comp)\n",
    "\n",
    "  # Assign label to each cluster based on the medoid of the cluster\n",
    "  cluster_to_label = dict()\n",
    "  for cluster in range(k):\n",
    "    centroid = kmeans.cluster_centers_[cluster]\n",
    "    cluster_imgs = x_train_comp[np.where(kmeans.labels_ == cluster)]\n",
    "    cluster_dists = [mean_squared_error(centroid, img) for img in cluster_imgs]\n",
    "    cluster_dist_min = min(cluster_dists)\n",
    "    medoid_idx = cluster_dists.index(cluster_dist_min)\n",
    "    medoid_img = cluster_imgs[medoid_idx]\n",
    "    medoid_label = img_to_label[str(medoid_img)]\n",
    "    cluster_to_label[cluster] = medoid_label\n",
    "\n",
    "  # Predict labels and get accuracy\n",
    "  cluster_labels = kmeans.predict(x_test_comp)\n",
    "  preds = np.array(list(map(lambda n: cluster_to_label[n], cluster_labels)))\n",
    "  acc = accuracy_score(y_test, preds)\n",
    "\n",
    "  accs.append((k, acc))\n",
    "  print(str(k) + '\\t' + str(acc))\n",
    "\n",
    "  if acc < 0.9:\n",
    "    k *= 2 # Double number of clusters with each iteration\n",
    "  else:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NKiSgfHpW4X7",
    "outputId": "7799541b-94ba-43f2-8551-2d01f5f28101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310\n"
     ]
    }
   ],
   "source": [
    "imgs_queried = sum([t[0] for t in accs])\n",
    "print(imgs_queried)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "3o7nOtsUaSy1",
    "outputId": "8059040a-f22e-491d-83f1-3875fb9175f5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xs = list(map(lambda t: t[0], accs))\n",
    "ys = list(map(lambda t: t[1], accs))\n",
    "\n",
    "plt.scatter(xs, ys)\n",
    "plt.plot([0, accs[-1][0]], [0.9, 0.9], 'k:', lw=2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "MNIST_Unsupervised.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
